{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ianara01/Park-Sangjin/blob/master/9_lenet_answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eonE9pT8Rdqa"
      },
      "source": [
        "#**Mounting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOi-tkXWRbqm"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7b5QonTRjzR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e7f7105c-1896-4aaa-badd-166f5d90a464"
      },
      "source": [
        "import os\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/ML_class/2023/6_CNN')\n",
        "%pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/ML_class/2023/6_CNN'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tCipQ-4nGpr"
      },
      "source": [
        "# Convolutional Neural Networks: MNIST\n",
        "\n",
        "In this notebook, you will:\n",
        "\n",
        "- Implement helper functions that you will use when implementing a PyTorch model\n",
        "- Implement a fully functioning ConvNet using PyTorch\n",
        "\n",
        "**After this assignment you will be able to:**\n",
        "\n",
        "- Build and train a ConvNet in PyTorch for a classification problem\n",
        "\n",
        "We assume here that you are already familiar with PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aatpvdbgms50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2648c1a4-d01d-418d-ec8b-a5c2feacd680"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('using device:', device)\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**2,1), 'MB')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n",
            "Tesla T4\n",
            "Memory usage:\n",
            "Allocated: 0.0 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BKpoUNNnQxA"
      },
      "source": [
        "## 1. PyTorch model\n",
        "\n",
        "### 1.1. Load dataset (MNIST)\n",
        "\n",
        "Most practical applications of deep learning today are built using programming frameworks, which have many built-in functions you can simply call.\n",
        "\n",
        "We will start by loading in the packages.\n",
        "\n",
        "The MNIST Dataset is located at http://yann.lecun.com/exdb/mnist/. Each image has $28 \\times 28$ dimension.\n",
        "\n",
        "-train-images-idx3-ubyte.gz:  training set images (9912422 bytes) including 55000 examples <br>\n",
        "-train-labels-idx1-ubyte.gz:  training set labels (28881 bytes) <br>\n",
        "-t10k-images-idx3-ubyte.gz:   test set images (1648877 bytes) including 10000 examples <br>\n",
        "-t10k-labels-idx1-ubyte.gz:   test set labels (4542 bytes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFF0e5udms54"
      },
      "source": [
        "train_dataset = datasets.MNIST( root='./mnist_data/',\n",
        "                              train=True,\n",
        "                              transform=transforms.ToTensor(),\n",
        "                              download=True)\n",
        "test_dataset = datasets.MNIST( root='./mnist_data/',\n",
        "                             train = False,\n",
        "                             transform=transforms.ToTensor())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f\"x.shape: {train_dataset.__getitem__(0)[0].shape}\")\n",
        "print(f\"label: {train_dataset.__getitem__(0)[1]}\")\n",
        "plt.imshow(train_dataset.__getitem__(0)[0].squeeze(), cmap=\"gray\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "WC983IEZKxA7",
        "outputId": "0a71d92a-cc4e-43bb-f0bc-3e2b338326b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.shape: torch.Size([1, 28, 28])\n",
            "label: 5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fd770e23190>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbe0lEQVR4nO3df2xV9f3H8dflR6+I7e1KbW8rPyygsIlgxqDrVMRRKd1G5McWdS7BzWhwrRGYuNRM0W2uDqczbEz5Y4GxCSjJgEEWNi22ZLNgQBgxbg0l3VpGWyZb7y2FFmw/3z+I98uVFjyXe/u+vTwfySeh955378fjtU9vezn1OeecAADoZ4OsNwAAuDIRIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYGKI9QY+qaenR8eOHVN6erp8Pp/1dgAAHjnn1N7ervz8fA0a1PfrnKQL0LFjxzRq1CjrbQAALlNTU5NGjhzZ5/1J9y249PR06y0AAOLgUl/PExag1atX6/rrr9dVV12lwsJCvfvuu59qjm+7AUBquNTX84QE6PXXX9eyZcu0YsUKvffee5oyZYpKSkp0/PjxRDwcAGAgcgkwffp0V1ZWFvm4u7vb5efnu8rKykvOhkIhJ4nFYrFYA3yFQqGLfr2P+yugM2fOaP/+/SouLo7cNmjQIBUXF6u2tvaC47u6uhQOh6MWACD1xT1AH374obq7u5Wbmxt1e25urlpaWi44vrKyUoFAILJ4BxwAXBnM3wVXUVGhUCgUWU1NTdZbAgD0g7j/PaDs7GwNHjxYra2tUbe3trYqGAxecLzf75ff74/3NgAASS7ur4DS0tI0depUVVVVRW7r6elRVVWVioqK4v1wAIABKiFXQli2bJkWLVqkL3zhC5o+fbpefvlldXR06Nvf/nYiHg4AMAAlJED33HOP/vOf/+jpp59WS0uLbrnlFu3cufOCNyYAAK5cPuecs97E+cLhsAKBgPU2AACXKRQKKSMjo8/7zd8FBwC4MhEgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmhlhvAEgmgwcP9jwTCAQSsJP4KC8vj2nu6quv9jwzYcIEzzNlZWWeZ372s595nrnvvvs8z0hSZ2en55nnn3/e88yzzz7reSYV8AoIAGCCAAEATMQ9QM8884x8Pl/UmjhxYrwfBgAwwCXkZ0A33XST3nrrrf9/kCH8qAkAEC0hZRgyZIiCwWAiPjUAIEUk5GdAhw8fVn5+vsaOHav7779fjY2NfR7b1dWlcDgctQAAqS/uASosLNS6deu0c+dOvfLKK2poaNDtt9+u9vb2Xo+vrKxUIBCIrFGjRsV7SwCAJBT3AJWWluob3/iGJk+erJKSEv3xj39UW1ub3njjjV6Pr6ioUCgUiqympqZ4bwkAkIQS/u6AzMxM3Xjjjaqvr+/1fr/fL7/fn+htAACSTML/HtDJkyd15MgR5eXlJfqhAAADSNwD9Pjjj6umpkb//Oc/9c4772j+/PkaPHhwzJfCAACkprh/C+7o0aO67777dOLECV177bW67bbbtGfPHl177bXxfigAwAAW9wBt2rQp3p8SSWr06NGeZ9LS0jzPfOlLX/I8c9ttt3mekc79zNKrhQsXxvRYqebo0aOeZ1atWuV5Zv78+Z5n+noX7qX87W9/8zxTU1MT02NdibgWHADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgwuecc9abOF84HFYgELDexhXllltuiWlu165dnmf4dzsw9PT0eJ75zne+43nm5MmTnmdi0dzcHNPc//73P88zdXV1MT1WKgqFQsrIyOjzfl4BAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwMQQ6w3AXmNjY0xzJ06c8DzD1bDP2bt3r+eZtrY2zzN33nmn5xlJOnPmjOeZ3/72tzE9Fq5cvAICAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAExwMVLov//9b0xzy5cv9zzzta99zfPMgQMHPM+sWrXK80ysDh486Hnmrrvu8jzT0dHheeamm27yPCNJjz32WExzgBe8AgIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATPicc856E+cLh8MKBALW20CCZGRkeJ5pb2/3PLNmzRrPM5L04IMPep751re+5Xlm48aNnmeAgSYUCl30v3leAQEATBAgAIAJzwHavXu35s6dq/z8fPl8Pm3dujXqfuecnn76aeXl5WnYsGEqLi7W4cOH47VfAECK8Bygjo4OTZkyRatXr+71/pUrV2rVqlV69dVXtXfvXg0fPlwlJSXq7Oy87M0CAFKH59+IWlpaqtLS0l7vc87p5Zdf1g9+8APdfffdkqT169crNzdXW7du1b333nt5uwUApIy4/gyooaFBLS0tKi4ujtwWCARUWFio2traXme6uroUDoejFgAg9cU1QC0tLZKk3NzcqNtzc3Mj931SZWWlAoFAZI0aNSqeWwIAJCnzd8FVVFQoFApFVlNTk/WWAAD9IK4BCgaDkqTW1tao21tbWyP3fZLf71dGRkbUAgCkvrgGqKCgQMFgUFVVVZHbwuGw9u7dq6Kiong+FABggPP8LriTJ0+qvr4+8nFDQ4MOHjyorKwsjR49WkuWLNGPf/xj3XDDDSooKNBTTz2l/Px8zZs3L577BgAMcJ4DtG/fPt15552Rj5ctWyZJWrRokdatW6cnnnhCHR0devjhh9XW1qbbbrtNO3fu1FVXXRW/XQMABjwuRoqU9MILL8Q09/H/UHlRU1Pjeeb8v6rwafX09HieASxxMVIAQFIiQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACa6GjZQ0fPjwmOa2b9/ueeaOO+7wPFNaWup55s9//rPnGcASV8MGACQlAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEFyMFzjNu3DjPM++9957nmba2Ns8zb7/9tueZffv2eZ6RpNWrV3ueSbIvJUgCXIwUAJCUCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATXIwUuEzz58/3PLN27VrPM+np6Z5nYvXkk096nlm/fr3nmebmZs8zGDi4GCkAICkRIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACa4GClgYNKkSZ5nXnrpJc8zs2bN8jwTqzVr1nieee655zzP/Pvf//Y8AxtcjBQAkJQIEADAhOcA7d69W3PnzlV+fr58Pp+2bt0adf8DDzwgn88XtebMmROv/QIAUoTnAHV0dGjKlClavXp1n8fMmTNHzc3NkbVx48bL2iQAIPUM8TpQWlqq0tLSix7j9/sVDAZj3hQAIPUl5GdA1dXVysnJ0YQJE/TII4/oxIkTfR7b1dWlcDgctQAAqS/uAZozZ47Wr1+vqqoq/fSnP1VNTY1KS0vV3d3d6/GVlZUKBAKRNWrUqHhvCQCQhDx/C+5S7r333sifb775Zk2ePFnjxo1TdXV1r38noaKiQsuWLYt8HA6HiRAAXAES/jbssWPHKjs7W/X19b3e7/f7lZGREbUAAKkv4QE6evSoTpw4oby8vEQ/FABgAPH8LbiTJ09GvZppaGjQwYMHlZWVpaysLD377LNauHChgsGgjhw5oieeeELjx49XSUlJXDcOABjYPAdo3759uvPOOyMff/zzm0WLFumVV17RoUOH9Jvf/EZtbW3Kz8/X7Nmz9aMf/Uh+vz9+uwYADHhcjBQYIDIzMz3PzJ07N6bHWrt2recZn8/neWbXrl2eZ+666y7PM7DBxUgBAEmJAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJrgaNoALdHV1eZ4ZMsTzb3fRRx995Hkmlt8tVl1d7XkGl4+rYQMAkhIBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYML71QMBXLbJkyd7nvn617/ueWbatGmeZ6TYLiwaiw8++MDzzO7duxOwE1jgFRAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIKLkQLnmTBhgueZ8vJyzzMLFizwPBMMBj3P9Kfu7m7PM83NzZ5nenp6PM8gOfEKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwwcVIkfRiuQjnfffdF9NjxXJh0euvvz6mx0pm+/bt8zzz3HPPeZ75wx/+4HkGqYNXQAAAEwQIAGDCU4AqKys1bdo0paenKycnR/PmzVNdXV3UMZ2dnSorK9OIESN0zTXXaOHChWptbY3rpgEAA5+nANXU1KisrEx79uzRm2++qbNnz2r27Nnq6OiIHLN06VJt375dmzdvVk1NjY4dOxbTL98CAKQ2T29C2LlzZ9TH69atU05Ojvbv368ZM2YoFArp17/+tTZs2KAvf/nLkqS1a9fqs5/9rPbs2aMvfvGL8ds5AGBAu6yfAYVCIUlSVlaWJGn//v06e/asiouLI8dMnDhRo0ePVm1tba+fo6urS+FwOGoBAFJfzAHq6enRkiVLdOutt2rSpEmSpJaWFqWlpSkzMzPq2NzcXLW0tPT6eSorKxUIBCJr1KhRsW4JADCAxBygsrIyvf/++9q0adNlbaCiokKhUCiympqaLuvzAQAGhpj+Imp5ebl27Nih3bt3a+TIkZHbg8Ggzpw5o7a2tqhXQa2trX3+ZUK/3y+/3x/LNgAAA5inV0DOOZWXl2vLli3atWuXCgoKou6fOnWqhg4dqqqqqshtdXV1amxsVFFRUXx2DABICZ5eAZWVlWnDhg3atm2b0tPTIz/XCQQCGjZsmAKBgB588EEtW7ZMWVlZysjI0KOPPqqioiLeAQcAiOIpQK+88ookaebMmVG3r127Vg888IAk6ec//7kGDRqkhQsXqqurSyUlJfrVr34Vl80CAFKHzznnrDdxvnA4rEAgYL0NfAq5ubmeZz73uc95nvnlL3/peWbixImeZ5Ld3r17Pc+88MILMT3Wtm3bPM/09PTE9FhIXaFQSBkZGX3ez7XgAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYCKm34iK5JWVleV5Zs2aNTE91i233OJ5ZuzYsTE9VjJ75513PM+8+OKLnmf+9Kc/eZ45ffq05xmgv/AKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwwcVI+0lhYaHnmeXLl3uemT59uueZ6667zvNMsjt16lRMc6tWrfI885Of/MTzTEdHh+cZINXwCggAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMMHFSPvJ/Pnz+2WmP33wwQeeZ3bs2OF55qOPPvI88+KLL3qekaS2traY5gB4xysgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMCEzznnrDdxvnA4rEAgYL0NAMBlCoVCysjI6PN+XgEBAEwQIACACU8Bqqys1LRp05Senq6cnBzNmzdPdXV1UcfMnDlTPp8vai1evDiumwYADHyeAlRTU6OysjLt2bNHb775ps6ePavZs2ero6Mj6riHHnpIzc3NkbVy5cq4bhoAMPB5+o2oO3fujPp43bp1ysnJ0f79+zVjxozI7VdffbWCwWB8dggASEmX9TOgUCgkScrKyoq6/bXXXlN2drYmTZqkiooKnTp1qs/P0dXVpXA4HLUAAFcAF6Pu7m731a9+1d16661Rt69Zs8bt3LnTHTp0yP3ud79z1113nZs/f36fn2fFihVOEovFYrFSbIVCoYt2JOYALV682I0ZM8Y1NTVd9LiqqionydXX1/d6f2dnpwuFQpHV1NRkftJYLBaLdfnrUgHy9DOgj5WXl2vHjh3avXu3Ro4cedFjCwsLJUn19fUaN27cBff7/X75/f5YtgEAGMA8Bcg5p0cffVRbtmxRdXW1CgoKLjlz8OBBSVJeXl5MGwQApCZPASorK9OGDRu0bds2paenq6WlRZIUCAQ0bNgwHTlyRBs2bNBXvvIVjRgxQocOHdLSpUs1Y8YMTZ48OSH/AACAAcrLz33Ux/f51q5d65xzrrGx0c2YMcNlZWU5v9/vxo8f75YvX37J7wOeLxQKmX/fksVisViXvy71tZ+LkQIAEoKLkQIAkhIBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwETSBcg5Z70FAEAcXOrredIFqL293XoLAIA4uNTXc59LspccPT09OnbsmNLT0+Xz+aLuC4fDGjVqlJqampSRkWG0Q3uch3M4D+dwHs7hPJyTDOfBOaf29nbl5+dr0KC+X+cM6cc9fSqDBg3SyJEjL3pMRkbGFf0E+xjn4RzOwzmch3M4D+dYn4dAIHDJY5LuW3AAgCsDAQIAmBhQAfL7/VqxYoX8fr/1VkxxHs7hPJzDeTiH83DOQDoPSfcmBADAlWFAvQICAKQOAgQAMEGAAAAmCBAAwMSACdDq1at1/fXX66qrrlJhYaHeffdd6y31u2eeeUY+ny9qTZw40XpbCbd7927NnTtX+fn58vl82rp1a9T9zjk9/fTTysvL07Bhw1RcXKzDhw/bbDaBLnUeHnjggQueH3PmzLHZbIJUVlZq2rRpSk9PV05OjubNm6e6urqoYzo7O1VWVqYRI0bommuu0cKFC9Xa2mq048T4NOdh5syZFzwfFi9ebLTj3g2IAL3++utatmyZVqxYoffee09TpkxRSUmJjh8/br21fnfTTTepubk5sv7yl79YbynhOjo6NGXKFK1evbrX+1euXKlVq1bp1Vdf1d69ezV8+HCVlJSos7Ozn3eaWJc6D5I0Z86cqOfHxo0b+3GHiVdTU6OysjLt2bNHb775ps6ePavZs2ero6MjcszSpUu1fft2bd68WTU1NTp27JgWLFhguOv4+zTnQZIeeuihqOfDypUrjXbcBzcATJ8+3ZWVlUU+7u7udvn5+a6ystJwV/1vxYoVbsqUKdbbMCXJbdmyJfJxT0+PCwaD7oUXXojc1tbW5vx+v9u4caPBDvvHJ8+Dc84tWrTI3X333Sb7sXL8+HEnydXU1Djnzv27Hzp0qNu8eXPkmL///e9OkqutrbXaZsJ98jw459wdd9zhHnvsMbtNfQpJ/wrozJkz2r9/v4qLiyO3DRo0SMXFxaqtrTXcmY3Dhw8rPz9fY8eO1f3336/GxkbrLZlqaGhQS0tL1PMjEAiosLDwinx+VFdXKycnRxMmTNAjjzyiEydOWG8poUKhkCQpKytLkrR//36dPXs26vkwceJEjR49OqWfD588Dx977bXXlJ2drUmTJqmiokKnTp2y2F6fku5ipJ/04Ycfqru7W7m5uVG35+bm6h//+IfRrmwUFhZq3bp1mjBhgpqbm/Xss8/q9ttv1/vvv6/09HTr7ZloaWmRpF6fHx/fd6WYM2eOFixYoIKCAh05ckRPPvmkSktLVVtbq8GDB1tvL+56enq0ZMkS3XrrrZo0aZKkc8+HtLQ0ZWZmRh2bys+H3s6DJH3zm9/UmDFjlJ+fr0OHDun73/++6urq9Pvf/95wt9GSPkD4f6WlpZE/T548WYWFhRozZozeeOMNPfjgg4Y7QzK49957I3+++eabNXnyZI0bN07V1dWaNWuW4c4So6ysTO+///4V8XPQi+nrPDz88MORP998883Ky8vTrFmzdOTIEY0bN66/t9mrpP8WXHZ2tgYPHnzBu1haW1sVDAaNdpUcMjMzdeONN6q+vt56K2Y+fg7w/LjQ2LFjlZ2dnZLPj/Lycu3YsUNvv/121K9vCQaDOnPmjNra2qKOT9XnQ1/noTeFhYWSlFTPh6QPUFpamqZOnaqqqqrIbT09PaqqqlJRUZHhzuydPHlSR44cUV5envVWzBQUFCgYDEY9P8LhsPbu3XvFPz+OHj2qEydOpNTzwzmn8vJybdmyRbt27VJBQUHU/VOnTtXQoUOjng91dXVqbGxMqefDpc5Dbw4ePChJyfV8sH4XxKexadMm5/f73bp169wHH3zgHn74YZeZmelaWlqst9avvve977nq6mrX0NDg/vrXv7ri4mKXnZ3tjh8/br21hGpvb3cHDhxwBw4ccJLcSy+95A4cOOD+9a9/Oeece/75511mZqbbtm2bO3TokLv77rtdQUGBO336tPHO4+ti56G9vd09/vjjrra21jU0NLi33nrLff7zn3c33HCD6+zstN563DzyyCMuEAi46upq19zcHFmnTp2KHLN48WI3evRot2vXLrdv3z5XVFTkioqKDHcdf5c6D/X19e6HP/yh27dvn2toaHDbtm1zY8eOdTNmzDDeebQBESDnnPvFL37hRo8e7dLS0tz06dPdnj17rLfU7+655x6Xl5fn0tLS3HXXXefuueceV19fb72thHv77bedpAvWokWLnHPn3or91FNPudzcXOf3+92sWbNcXV2d7aYT4GLn4dSpU2727Nnu2muvdUOHDnVjxoxxDz30UMr9T1pv//yS3Nq1ayPHnD592n33u991n/nMZ9zVV1/t5s+f75qbm+02nQCXOg+NjY1uxowZLisry/n9fjd+/Hi3fPlyFwqFbDf+Cfw6BgCAiaT/GRAAIDURIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACb+Dwuo74MxItlsAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwkQNi5foOTO"
      },
      "source": [
        "### 1.1. Prepare dataset\n",
        "\n",
        "Let's prepare the dataset using **torch.tuils.data.Dataloader**\n",
        "\n",
        "PyTorch provides DataLoader for the input data (training/testing) that will be fed into the model when training/testing the model.\n",
        "\n",
        "**Exercise**: Implement the function below to prepare the training/testing data. You can define the number of examples for the moment and shuffle the order of examples by setting batch_size and shuffle.\n",
        "\n",
        "**The constructor arguments of a DataLoader :**\n",
        "```python\n",
        "torch.utils.data.DataLoader (dataset, batch_size=1, shuffle=False, sampler=None,\n",
        "                            batch_sampler=None, num_workers=0, collate_fn=None,\n",
        "                            pin_memory=False, drop_last=False, timeout=0,\n",
        "                            worker_init_fn=None)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDNXScGRoIEb"
      },
      "source": [
        "### Set the batch_size\n",
        "batch_size = 64\n",
        "\n",
        "### START CODE HERE ### (â‰ˆ1 lines)\n",
        "#Use torch.utils.data.DataLoader() with bath_size 64, shuffle = True\n",
        "train_loader = torch.utils.data.DataLoader( dataset = train_dataset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = True)\n",
        "### ENd CODE HERE ###\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader( dataset=test_dataset,\n",
        "                                          batch_size = batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "# train_loader has (x, y) where x.size()=[64, 1, 28, 28] and y.size()=[64]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpdTftvIpRFs"
      },
      "source": [
        "### 1.2. Forward propagation\n",
        "\n",
        "In PyTorch, there are built-in functions that carry out the convolution steps for you.\n",
        "\n",
        "- **torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'):** Applies a 2D convolution over an input signal composed of several input planes.  \n",
        "\n",
        "- **torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False):** applies a 2D max pooling over an input signal composed of several input planes.\n",
        "\n",
        "- **torch.nn.Linear(in_features, out_features, bias=True):** Applies a linear transformation to the incoming data\n",
        "\n",
        "- **torch.nn.functional.relu(input, inplace=False):** Applies the rectified linear unit function element-wise\n",
        "\n",
        "- **torch.nn.functional.softmax(dim=None):** torch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None)\n",
        "\n",
        "- **torch.flatten(input, start_dim=0, end_dim=-1) :** Flattens a contiguous range of dims in a tensor.\n",
        "\n",
        "**For more details:**\n",
        "\n",
        "torch.nn : https://pytorch.org/docs/stable/nn.html\n",
        "\n",
        "torch.functional : https://pytorch.org/docs/stable/nn.functional.html\n",
        "\n",
        "torch.flatten : https://pytorch.org/docs/master/generated/torch.flatten.html\n",
        "\n",
        "\n",
        "**Exercise**:\n",
        "\n",
        "Implement the `NNModel` class below to build the following model: `CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED`. You should use the functions above.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bDY8CEgms6f"
      },
      "source": [
        "\"\"\"\n",
        "Implements forward propabation for the CNN model:\n",
        "CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
        "\n",
        "\"\"\"\n",
        "class NNModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 5, padding=(2,2)) # input channel, output channels, and filter size\n",
        "        self.pool1 = nn.MaxPool2d(2, stride=2)\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "        # Define the Convolution layer(Condv2d) with (32:input channel, 64:output channels, 5: filter size) and the 2 padding.\n",
        "        self.conv2 = nn.Conv2d(32, 64, 5, padding=(2,2))\n",
        "        # Define the Maxpooling layer(MaxPool2d) with (2:filter size, 2:stride)\n",
        "        self.pool2 = nn.MaxPool2d(2, stride=2)\n",
        "        # Define the fully-conneted layer(Linear) with (3136:input features, 1024:output features)\n",
        "        self.fc1 = nn.Linear(3136,1024)\n",
        "        ### END CODE HERE ###\n",
        "        self.fc2 = nn.Linear(1024,10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "            Arguments:\n",
        "            X -- input dataset placeholder, of shape (input size, number of examples)\n",
        "            parameters -- python dictionary containing your parameters \"W1\", \"W2\"\n",
        "                          the shapes are given in initialize_parameters\n",
        "\n",
        "            Returns:\n",
        "            F.softmax(x) -- the softmax output of the last LINEAR unit\n",
        "        \"\"\"\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        ### START CODE HERE ###\n",
        "        # Load sceond Convoultion layer(conv2d)\n",
        "        x = self.conv2(x)\n",
        "        # Load Relu function\n",
        "        x = F.relu(x)\n",
        "        # Load sceond Maxpooling layer(MaxPool2d)\n",
        "        x = self.pool2(x)\n",
        "        # Flatten each example into a 1D vector: (?,7,7,64)->(?, 3136)\n",
        "        x = torch.flatten(x, 1)\n",
        "        # Load first fully-connted layer\n",
        "        x = self.fc1(x)\n",
        "        ### END CODE HERE ###\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "#        return F.softmax(x, dim=1)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 0\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "temp_model = NNModel()\n",
        "\n",
        "def weights_init_for_test(m):\n",
        "  if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "    torch.nn.init.zeros_(m.weight)\n",
        "    torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "temp_model.apply(weights_init_for_test)\n",
        "\n",
        "temp_in = train_dataset.__getitem__(0)[0]\n",
        "temp_out = temp_model(temp_in.unsqueeze(0))\n",
        "print(temp_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD5iIkuyRv7B",
        "outputId": "79abce7b-1799-4fa9-ca48-b227f284092d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=\\<AddmmBackward0\\>)"
      ],
      "metadata": {
        "id": "QIV6lEKJTjXE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvmWxnal0hWA"
      },
      "source": [
        "### 1.3. Build model\n",
        "\n",
        "Finally you will merge the helper functions you implemented above to build and train a model.  \n",
        "\n",
        "**Exercise**: Complete the function below.\n",
        "\n",
        "The model below should:\n",
        "\n",
        "- Define the cost function with **torch.nn.CrossEntropyLoss()**    \n",
        "> torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n",
        "> **For more details:** https://pytorch.org/docs/stable/nn.html\n",
        "\n",
        "- Create optmizization function with **torch.optim.SGD()**\n",
        "> torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)     \n",
        "> **For more details:** https://pytorch.org/docs/stable/optim.html\n",
        "- hint : For getting params for optmizer, use the following code\n",
        "```python\n",
        "params = model.parameters()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "akEyMJMxms6i"
      },
      "source": [
        "model = NNModel()\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "model.apply(weights_init)\n",
        "\n",
        "### START CODE HERE ###\n",
        "# Define the cost function with cross entropy\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Create optimizer\n",
        "# Get model paramters\n",
        "params = model.parameters()\n",
        "# Define the optimizer (using SGD) with learing rate=0.01 and momentum=0.5\n",
        "optimizer = optim.SGD(params, lr=0.01, momentum=0.5)\n",
        "### END CODE HERE ###\n",
        "\n",
        "model = model.to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "701_v22n3KxE"
      },
      "source": [
        "## 2. Train and test model\n",
        "\n",
        "Finally you will create training and testing function with a loop for each mini-batch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-cfYlJx4ig1"
      },
      "source": [
        "### 2.1 Train function\n",
        "\n",
        "Implement the function to train the CNN model.\n",
        "\n",
        "**Exercise**: Complete the function below.\n",
        "\n",
        "- copy a tensor on the CPU to the GPU\n",
        "- call the CNN model\n",
        "- initialize the optimizer  \n",
        "- compute the cost  \n",
        "- compute the backward propagation\n",
        "- update parameters with the optimizer\n",
        "\n",
        "**Hint**\n",
        "- copy a tensor on the CPU to the GPU\n",
        "         # copy a tensor to the CPU\n",
        "         data = data.to(\"cpu\")\n",
        "         # copy a tensor to the GPU\n",
        "         data = data.to(\"cuda\")\n",
        "- call the CNN model : call the predefined CNN model funciton    \n",
        "- initialize optimizer\n",
        "        optimizer.zero_grad()\n",
        "- compute the cost : call the predifined criterion function  \n",
        "- compute the backward propagation   \n",
        "        loss.backward()\n",
        "- update parameters with the optimizer\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40y8dtLi24_0"
      },
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "        # Copy \"data\" tensor to the GPU\n",
        "        data = data.to(device)\n",
        "        # Copy \"target\" tensor to the GPU\n",
        "        target = target.to(device)\n",
        "        # Call model function and input the data tensor\n",
        "        output = model(data)\n",
        "        # Initialize the optimizer\n",
        "        optimizer.zero_grad()\n",
        "        # Compute the cost\n",
        "        loss = criterion(output, target)\n",
        "        # Compute the back-prop\n",
        "        loss.backward()\n",
        "        # Update prameters with the optmizer\n",
        "        optimizer.step()\n",
        "        ### END CODE HERE ###\n",
        "        if batch_idx%100==0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch,\n",
        "                batch_idx*len(data),\n",
        "                len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqMzKBR_4xCq"
      },
      "source": [
        "### 2.2 Test function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXIwfghqms6n"
      },
      "source": [
        "def test():\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_loss=0\n",
        "        correct = 0\n",
        "        for data, target in train_loader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output,target)\n",
        "            pred = output.data.max(1,keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "        test_loss /=len(train_loader.dataset)\n",
        "        print('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "            test_loss, correct,\n",
        "            len(train_loader.dataset),\n",
        "            100. * correct / len(train_loader.dataset)))\n",
        "\n",
        "        test_loss=0\n",
        "        correct = 0\n",
        "        for data, target in test_loader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output,target)\n",
        "            pred = output.data.max(1,keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "        test_loss /=len(test_loader.dataset)\n",
        "        print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            test_loss, correct,\n",
        "            len(test_loader.dataset),\n",
        "            100. * correct / len(test_loader.dataset)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xleeHtag4DKl"
      },
      "source": [
        "### 2.3. Run the train/test function\n",
        "\n",
        "Run the following cell to train and test your model for 100 epochs.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5McB7DF1D5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1d993ce-0835-4bf4-bc85-4d73a7cbfcdc"
      },
      "source": [
        "for epoch in range(0,100):\n",
        "    train(epoch)\n",
        "    if epoch%100==0:\n",
        "        test()\n",
        "test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.302463\n",
            "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.260412\n",
            "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.687199\n",
            "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.505515\n",
            "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.468251\n",
            "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.293451\n",
            "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.296168\n",
            "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.184634\n",
            "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.198093\n",
            "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.194800\n",
            "Train set: Average loss: 0.0031, Accuracy: 56333/60000 (94%)\n",
            "Test set: Average loss: 0.0028, Accuracy: 9418/10000 (94%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.169697\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.562916\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.135246\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.138963\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.078383\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.054550\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.267101\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.071807\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.028441\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.058610\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.077260\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.124085\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.069665\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.115146\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.142271\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.054142\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.079342\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.017437\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.034498\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.057646\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.133357\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.019675\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.087817\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.011812\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.068376\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.031009\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.045494\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.092533\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.078539\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.123478\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.070268\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.035207\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.017605\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.031565\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.070853\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.027681\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.122104\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.036060\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.108049\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.015861\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.049592\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.035985\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.026687\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.047630\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.061855\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.023838\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.095284\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.095931\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.037831\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.021319\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.011459\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.074992\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.060057\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.056790\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.026993\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.082486\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.034252\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.025304\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.009414\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.066043\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.006221\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.133705\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.019239\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.004814\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.007069\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.006248\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.069371\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.015655\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.062072\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.004432\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.070923\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.022963\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.014519\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.012586\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.013810\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.017993\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.006090\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.101102\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.005850\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.033309\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.019719\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.040924\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.061206\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.003862\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.068279\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.007209\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.044154\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.076757\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.020285\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.062820\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.003514\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.010018\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.026305\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.017608\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.008929\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.021505\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.012265\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.093472\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.046604\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.002067\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.006609\n",
            "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.017863\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.034581\n",
            "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.024566\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.019640\n",
            "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.004513\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.009502\n",
            "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.011658\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.021945\n",
            "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.005937\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.009048\n",
            "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.030200\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.001694\n",
            "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.022664\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.008827\n",
            "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.008271\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.143794\n",
            "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.004415\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.094786\n",
            "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.038641\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.003359\n",
            "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.004377\n",
            "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.003162\n",
            "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.027990\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.019665\n",
            "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.044804\n",
            "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.042108\n",
            "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.006382\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.033055\n",
            "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.000252\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.029742\n",
            "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.004696\n",
            "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.012477\n",
            "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.075822\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.001234\n",
            "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.010240\n",
            "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.013769\n",
            "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.004387\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.018938\n",
            "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.001633\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.008260\n",
            "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 0.002220\n",
            "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.002190\n",
            "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 0.001768\n",
            "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.042188\n",
            "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.068104\n",
            "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.002550\n",
            "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 0.004290\n",
            "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.003744\n",
            "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.010204\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.023801\n",
            "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 0.002719\n",
            "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.014117\n",
            "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 0.003659\n",
            "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.004025\n",
            "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.001747\n",
            "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.084349\n",
            "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 0.003004\n",
            "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.010628\n",
            "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 0.010087\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.025310\n",
            "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 0.005950\n",
            "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.002189\n",
            "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 0.002423\n",
            "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.090017\n",
            "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.003640\n",
            "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.004260\n",
            "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 0.005887\n",
            "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.003308\n",
            "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 0.045305\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.002536\n",
            "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 0.010249\n",
            "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.000973\n",
            "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 0.250747\n",
            "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.036054\n",
            "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 0.016493\n",
            "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.001004\n",
            "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 0.017957\n",
            "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.004020\n",
            "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 0.002971\n",
            "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.010329\n",
            "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 0.023976\n",
            "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.010063\n",
            "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 0.001248\n",
            "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.089209\n",
            "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 0.005554\n",
            "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.022885\n",
            "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 0.018461\n",
            "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.006673\n",
            "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 0.000858\n",
            "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.006557\n",
            "Train Epoch: 20 [6400/60000 (11%)]\tLoss: 0.001830\n",
            "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 0.018263\n",
            "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 0.007862\n",
            "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.008490\n",
            "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 0.011895\n",
            "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.071540\n",
            "Train Epoch: 20 [44800/60000 (75%)]\tLoss: 0.004766\n",
            "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.003323\n",
            "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 0.037790\n",
            "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.012348\n",
            "Train Epoch: 21 [6400/60000 (11%)]\tLoss: 0.006261\n",
            "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 0.001949\n",
            "Train Epoch: 21 [19200/60000 (32%)]\tLoss: 0.005076\n",
            "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.003236\n",
            "Train Epoch: 21 [32000/60000 (53%)]\tLoss: 0.004823\n",
            "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 0.001423\n",
            "Train Epoch: 21 [44800/60000 (75%)]\tLoss: 0.001450\n",
            "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.002996\n",
            "Train Epoch: 21 [57600/60000 (96%)]\tLoss: 0.003813\n",
            "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.032194\n",
            "Train Epoch: 22 [6400/60000 (11%)]\tLoss: 0.000446\n",
            "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 0.000429\n",
            "Train Epoch: 22 [19200/60000 (32%)]\tLoss: 0.020830\n",
            "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.003937\n",
            "Train Epoch: 22 [32000/60000 (53%)]\tLoss: 0.004646\n",
            "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 0.011612\n",
            "Train Epoch: 22 [44800/60000 (75%)]\tLoss: 0.002989\n",
            "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.011624\n",
            "Train Epoch: 22 [57600/60000 (96%)]\tLoss: 0.024387\n",
            "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.004917\n",
            "Train Epoch: 23 [6400/60000 (11%)]\tLoss: 0.006158\n",
            "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 0.001949\n",
            "Train Epoch: 23 [19200/60000 (32%)]\tLoss: 0.003252\n",
            "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.002718\n",
            "Train Epoch: 23 [32000/60000 (53%)]\tLoss: 0.000322\n",
            "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 0.012658\n",
            "Train Epoch: 23 [44800/60000 (75%)]\tLoss: 0.001955\n",
            "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.005292\n",
            "Train Epoch: 23 [57600/60000 (96%)]\tLoss: 0.000212\n",
            "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.000440\n",
            "Train Epoch: 24 [6400/60000 (11%)]\tLoss: 0.007809\n",
            "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 0.024740\n",
            "Train Epoch: 24 [19200/60000 (32%)]\tLoss: 0.001344\n",
            "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.001865\n",
            "Train Epoch: 24 [32000/60000 (53%)]\tLoss: 0.003549\n",
            "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 0.003465\n",
            "Train Epoch: 24 [44800/60000 (75%)]\tLoss: 0.001959\n",
            "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 0.017098\n",
            "Train Epoch: 24 [57600/60000 (96%)]\tLoss: 0.003468\n",
            "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.013986\n",
            "Train Epoch: 25 [6400/60000 (11%)]\tLoss: 0.010016\n",
            "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 0.018542\n",
            "Train Epoch: 25 [19200/60000 (32%)]\tLoss: 0.001097\n",
            "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 0.002120\n",
            "Train Epoch: 25 [32000/60000 (53%)]\tLoss: 0.000786\n",
            "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 0.009157\n",
            "Train Epoch: 25 [44800/60000 (75%)]\tLoss: 0.008241\n",
            "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 0.004569\n",
            "Train Epoch: 25 [57600/60000 (96%)]\tLoss: 0.003994\n",
            "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.000324\n",
            "Train Epoch: 26 [6400/60000 (11%)]\tLoss: 0.001025\n",
            "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 0.013371\n",
            "Train Epoch: 26 [19200/60000 (32%)]\tLoss: 0.010667\n",
            "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 0.016128\n",
            "Train Epoch: 26 [32000/60000 (53%)]\tLoss: 0.043608\n",
            "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 0.006014\n",
            "Train Epoch: 26 [44800/60000 (75%)]\tLoss: 0.001580\n",
            "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 0.000458\n",
            "Train Epoch: 26 [57600/60000 (96%)]\tLoss: 0.023270\n",
            "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.002785\n",
            "Train Epoch: 27 [6400/60000 (11%)]\tLoss: 0.000450\n",
            "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 0.004065\n",
            "Train Epoch: 27 [19200/60000 (32%)]\tLoss: 0.000128\n",
            "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 0.005397\n",
            "Train Epoch: 27 [32000/60000 (53%)]\tLoss: 0.000770\n",
            "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 0.003099\n",
            "Train Epoch: 27 [44800/60000 (75%)]\tLoss: 0.002453\n",
            "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 0.001255\n",
            "Train Epoch: 27 [57600/60000 (96%)]\tLoss: 0.000255\n",
            "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.002476\n",
            "Train Epoch: 28 [6400/60000 (11%)]\tLoss: 0.003733\n",
            "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 0.001099\n",
            "Train Epoch: 28 [19200/60000 (32%)]\tLoss: 0.022464\n",
            "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 0.008040\n",
            "Train Epoch: 28 [32000/60000 (53%)]\tLoss: 0.002660\n",
            "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 0.004746\n",
            "Train Epoch: 28 [44800/60000 (75%)]\tLoss: 0.004617\n",
            "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 0.001033\n",
            "Train Epoch: 28 [57600/60000 (96%)]\tLoss: 0.001469\n",
            "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.000200\n",
            "Train Epoch: 29 [6400/60000 (11%)]\tLoss: 0.001532\n",
            "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 0.004077\n",
            "Train Epoch: 29 [19200/60000 (32%)]\tLoss: 0.003753\n",
            "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 0.005975\n",
            "Train Epoch: 29 [32000/60000 (53%)]\tLoss: 0.017967\n",
            "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 0.000999\n",
            "Train Epoch: 29 [44800/60000 (75%)]\tLoss: 0.000754\n",
            "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 0.000509\n",
            "Train Epoch: 29 [57600/60000 (96%)]\tLoss: 0.001306\n",
            "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.000694\n",
            "Train Epoch: 30 [6400/60000 (11%)]\tLoss: 0.006386\n",
            "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 0.002569\n",
            "Train Epoch: 30 [19200/60000 (32%)]\tLoss: 0.018499\n",
            "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 0.002557\n",
            "Train Epoch: 30 [32000/60000 (53%)]\tLoss: 0.000070\n",
            "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 0.004468\n",
            "Train Epoch: 30 [44800/60000 (75%)]\tLoss: 0.002518\n",
            "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 0.002291\n",
            "Train Epoch: 30 [57600/60000 (96%)]\tLoss: 0.001191\n",
            "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.016667\n",
            "Train Epoch: 31 [6400/60000 (11%)]\tLoss: 0.029259\n",
            "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 0.001249\n",
            "Train Epoch: 31 [19200/60000 (32%)]\tLoss: 0.001200\n",
            "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 0.004846\n",
            "Train Epoch: 31 [32000/60000 (53%)]\tLoss: 0.000478\n",
            "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 0.003941\n",
            "Train Epoch: 31 [44800/60000 (75%)]\tLoss: 0.012804\n",
            "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 0.012344\n",
            "Train Epoch: 31 [57600/60000 (96%)]\tLoss: 0.000831\n",
            "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.001973\n",
            "Train Epoch: 32 [6400/60000 (11%)]\tLoss: 0.000353\n",
            "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 0.002700\n",
            "Train Epoch: 32 [19200/60000 (32%)]\tLoss: 0.000486\n",
            "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 0.002452\n",
            "Train Epoch: 32 [32000/60000 (53%)]\tLoss: 0.001600\n",
            "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 0.001772\n",
            "Train Epoch: 32 [44800/60000 (75%)]\tLoss: 0.000768\n",
            "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 0.000136\n",
            "Train Epoch: 32 [57600/60000 (96%)]\tLoss: 0.004210\n",
            "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.000588\n",
            "Train Epoch: 33 [6400/60000 (11%)]\tLoss: 0.000456\n",
            "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 0.001994\n",
            "Train Epoch: 33 [19200/60000 (32%)]\tLoss: 0.000800\n",
            "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 0.001296\n",
            "Train Epoch: 33 [32000/60000 (53%)]\tLoss: 0.001579\n",
            "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 0.002049\n",
            "Train Epoch: 33 [44800/60000 (75%)]\tLoss: 0.000293\n",
            "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 0.005817\n",
            "Train Epoch: 33 [57600/60000 (96%)]\tLoss: 0.006235\n",
            "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.001379\n",
            "Train Epoch: 34 [6400/60000 (11%)]\tLoss: 0.000320\n",
            "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 0.000909\n",
            "Train Epoch: 34 [19200/60000 (32%)]\tLoss: 0.005894\n",
            "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 0.012441\n",
            "Train Epoch: 34 [32000/60000 (53%)]\tLoss: 0.005139\n",
            "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 0.001329\n",
            "Train Epoch: 34 [44800/60000 (75%)]\tLoss: 0.043481\n",
            "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 0.000321\n",
            "Train Epoch: 34 [57600/60000 (96%)]\tLoss: 0.000178\n",
            "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.004121\n",
            "Train Epoch: 35 [6400/60000 (11%)]\tLoss: 0.004100\n",
            "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 0.003525\n",
            "Train Epoch: 35 [19200/60000 (32%)]\tLoss: 0.000525\n",
            "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 0.003426\n",
            "Train Epoch: 35 [32000/60000 (53%)]\tLoss: 0.003636\n",
            "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 0.000812\n",
            "Train Epoch: 35 [44800/60000 (75%)]\tLoss: 0.000188\n",
            "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 0.000594\n",
            "Train Epoch: 35 [57600/60000 (96%)]\tLoss: 0.000905\n",
            "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.000544\n",
            "Train Epoch: 36 [6400/60000 (11%)]\tLoss: 0.004993\n",
            "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 0.001346\n",
            "Train Epoch: 36 [19200/60000 (32%)]\tLoss: 0.001296\n",
            "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 0.000713\n",
            "Train Epoch: 36 [32000/60000 (53%)]\tLoss: 0.000698\n",
            "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 0.001542\n",
            "Train Epoch: 36 [44800/60000 (75%)]\tLoss: 0.001994\n",
            "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 0.002076\n",
            "Train Epoch: 36 [57600/60000 (96%)]\tLoss: 0.007945\n",
            "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.016113\n",
            "Train Epoch: 37 [6400/60000 (11%)]\tLoss: 0.000750\n",
            "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 0.000459\n",
            "Train Epoch: 37 [19200/60000 (32%)]\tLoss: 0.000717\n",
            "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 0.000647\n",
            "Train Epoch: 37 [32000/60000 (53%)]\tLoss: 0.000918\n",
            "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 0.000946\n",
            "Train Epoch: 37 [44800/60000 (75%)]\tLoss: 0.000275\n",
            "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 0.003628\n",
            "Train Epoch: 37 [57600/60000 (96%)]\tLoss: 0.000180\n",
            "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.014868\n",
            "Train Epoch: 38 [6400/60000 (11%)]\tLoss: 0.000779\n",
            "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 0.001477\n",
            "Train Epoch: 38 [19200/60000 (32%)]\tLoss: 0.001919\n",
            "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 0.001076\n",
            "Train Epoch: 38 [32000/60000 (53%)]\tLoss: 0.002494\n",
            "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 0.010025\n",
            "Train Epoch: 38 [44800/60000 (75%)]\tLoss: 0.000567\n",
            "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 0.001022\n",
            "Train Epoch: 38 [57600/60000 (96%)]\tLoss: 0.000019\n",
            "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.001183\n",
            "Train Epoch: 39 [6400/60000 (11%)]\tLoss: 0.001049\n",
            "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 0.000438\n",
            "Train Epoch: 39 [19200/60000 (32%)]\tLoss: 0.001613\n",
            "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 0.000313\n",
            "Train Epoch: 39 [32000/60000 (53%)]\tLoss: 0.004400\n",
            "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 0.000128\n",
            "Train Epoch: 39 [44800/60000 (75%)]\tLoss: 0.000680\n",
            "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 0.015921\n",
            "Train Epoch: 39 [57600/60000 (96%)]\tLoss: 0.000142\n",
            "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.000731\n",
            "Train Epoch: 40 [6400/60000 (11%)]\tLoss: 0.000197\n",
            "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 0.001266\n",
            "Train Epoch: 40 [19200/60000 (32%)]\tLoss: 0.001381\n",
            "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 0.000634\n",
            "Train Epoch: 40 [32000/60000 (53%)]\tLoss: 0.000113\n",
            "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 0.000672\n",
            "Train Epoch: 40 [44800/60000 (75%)]\tLoss: 0.001969\n",
            "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 0.001192\n",
            "Train Epoch: 40 [57600/60000 (96%)]\tLoss: 0.000210\n",
            "Train Epoch: 41 [0/60000 (0%)]\tLoss: 0.000055\n",
            "Train Epoch: 41 [6400/60000 (11%)]\tLoss: 0.000078\n",
            "Train Epoch: 41 [12800/60000 (21%)]\tLoss: 0.000960\n",
            "Train Epoch: 41 [19200/60000 (32%)]\tLoss: 0.000425\n",
            "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 0.000488\n",
            "Train Epoch: 41 [32000/60000 (53%)]\tLoss: 0.001069\n",
            "Train Epoch: 41 [38400/60000 (64%)]\tLoss: 0.000942\n",
            "Train Epoch: 41 [44800/60000 (75%)]\tLoss: 0.000989\n",
            "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 0.000028\n",
            "Train Epoch: 41 [57600/60000 (96%)]\tLoss: 0.003767\n",
            "Train Epoch: 42 [0/60000 (0%)]\tLoss: 0.001359\n",
            "Train Epoch: 42 [6400/60000 (11%)]\tLoss: 0.000604\n",
            "Train Epoch: 42 [12800/60000 (21%)]\tLoss: 0.000093\n",
            "Train Epoch: 42 [19200/60000 (32%)]\tLoss: 0.001852\n",
            "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 0.000305\n",
            "Train Epoch: 42 [32000/60000 (53%)]\tLoss: 0.018904\n",
            "Train Epoch: 42 [38400/60000 (64%)]\tLoss: 0.000615\n",
            "Train Epoch: 42 [44800/60000 (75%)]\tLoss: 0.001180\n",
            "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 0.000483\n",
            "Train Epoch: 42 [57600/60000 (96%)]\tLoss: 0.001380\n",
            "Train Epoch: 43 [0/60000 (0%)]\tLoss: 0.000599\n",
            "Train Epoch: 43 [6400/60000 (11%)]\tLoss: 0.000544\n",
            "Train Epoch: 43 [12800/60000 (21%)]\tLoss: 0.000619\n",
            "Train Epoch: 43 [19200/60000 (32%)]\tLoss: 0.009960\n",
            "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 0.000504\n",
            "Train Epoch: 43 [32000/60000 (53%)]\tLoss: 0.000964\n",
            "Train Epoch: 43 [38400/60000 (64%)]\tLoss: 0.001471\n",
            "Train Epoch: 43 [44800/60000 (75%)]\tLoss: 0.001233\n",
            "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 0.001010\n",
            "Train Epoch: 43 [57600/60000 (96%)]\tLoss: 0.000223\n",
            "Train Epoch: 44 [0/60000 (0%)]\tLoss: 0.005754\n",
            "Train Epoch: 44 [6400/60000 (11%)]\tLoss: 0.005251\n",
            "Train Epoch: 44 [12800/60000 (21%)]\tLoss: 0.002571\n",
            "Train Epoch: 44 [19200/60000 (32%)]\tLoss: 0.000136\n",
            "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 0.000230\n",
            "Train Epoch: 44 [32000/60000 (53%)]\tLoss: 0.000411\n",
            "Train Epoch: 44 [38400/60000 (64%)]\tLoss: 0.000533\n",
            "Train Epoch: 44 [44800/60000 (75%)]\tLoss: 0.001260\n",
            "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 0.000147\n",
            "Train Epoch: 44 [57600/60000 (96%)]\tLoss: 0.000176\n",
            "Train Epoch: 45 [0/60000 (0%)]\tLoss: 0.000406\n",
            "Train Epoch: 45 [6400/60000 (11%)]\tLoss: 0.002067\n",
            "Train Epoch: 45 [12800/60000 (21%)]\tLoss: 0.001143\n",
            "Train Epoch: 45 [19200/60000 (32%)]\tLoss: 0.000726\n",
            "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 0.000881\n",
            "Train Epoch: 45 [32000/60000 (53%)]\tLoss: 0.000061\n",
            "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 0.000151\n",
            "Train Epoch: 45 [44800/60000 (75%)]\tLoss: 0.000311\n",
            "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 0.002233\n",
            "Train Epoch: 45 [57600/60000 (96%)]\tLoss: 0.001832\n",
            "Train Epoch: 46 [0/60000 (0%)]\tLoss: 0.001340\n",
            "Train Epoch: 46 [6400/60000 (11%)]\tLoss: 0.001828\n",
            "Train Epoch: 46 [12800/60000 (21%)]\tLoss: 0.000248\n",
            "Train Epoch: 46 [19200/60000 (32%)]\tLoss: 0.000185\n",
            "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 0.002504\n",
            "Train Epoch: 46 [32000/60000 (53%)]\tLoss: 0.000314\n",
            "Train Epoch: 46 [38400/60000 (64%)]\tLoss: 0.000376\n",
            "Train Epoch: 46 [44800/60000 (75%)]\tLoss: 0.001710\n",
            "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 0.000050\n",
            "Train Epoch: 46 [57600/60000 (96%)]\tLoss: 0.000498\n",
            "Train Epoch: 47 [0/60000 (0%)]\tLoss: 0.002613\n",
            "Train Epoch: 47 [6400/60000 (11%)]\tLoss: 0.000029\n",
            "Train Epoch: 47 [12800/60000 (21%)]\tLoss: 0.001471\n",
            "Train Epoch: 47 [19200/60000 (32%)]\tLoss: 0.000121\n",
            "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 0.001234\n",
            "Train Epoch: 47 [32000/60000 (53%)]\tLoss: 0.001678\n",
            "Train Epoch: 47 [38400/60000 (64%)]\tLoss: 0.000302\n",
            "Train Epoch: 47 [44800/60000 (75%)]\tLoss: 0.000288\n",
            "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 0.000750\n",
            "Train Epoch: 47 [57600/60000 (96%)]\tLoss: 0.000118\n",
            "Train Epoch: 48 [0/60000 (0%)]\tLoss: 0.000121\n",
            "Train Epoch: 48 [6400/60000 (11%)]\tLoss: 0.002558\n",
            "Train Epoch: 48 [12800/60000 (21%)]\tLoss: 0.000665\n",
            "Train Epoch: 48 [19200/60000 (32%)]\tLoss: 0.000421\n",
            "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 0.000410\n",
            "Train Epoch: 48 [32000/60000 (53%)]\tLoss: 0.000205\n",
            "Train Epoch: 48 [38400/60000 (64%)]\tLoss: 0.000621\n",
            "Train Epoch: 48 [44800/60000 (75%)]\tLoss: 0.001468\n",
            "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 0.000344\n",
            "Train Epoch: 48 [57600/60000 (96%)]\tLoss: 0.000015\n",
            "Train Epoch: 49 [0/60000 (0%)]\tLoss: 0.000254\n",
            "Train Epoch: 49 [6400/60000 (11%)]\tLoss: 0.000386\n",
            "Train Epoch: 49 [12800/60000 (21%)]\tLoss: 0.001032\n",
            "Train Epoch: 49 [19200/60000 (32%)]\tLoss: 0.000327\n",
            "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 0.001018\n",
            "Train Epoch: 49 [32000/60000 (53%)]\tLoss: 0.000370\n",
            "Train Epoch: 49 [38400/60000 (64%)]\tLoss: 0.000084\n",
            "Train Epoch: 49 [44800/60000 (75%)]\tLoss: 0.000170\n",
            "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 0.001113\n",
            "Train Epoch: 49 [57600/60000 (96%)]\tLoss: 0.001804\n",
            "Train Epoch: 50 [0/60000 (0%)]\tLoss: 0.000017\n",
            "Train Epoch: 50 [6400/60000 (11%)]\tLoss: 0.003286\n",
            "Train Epoch: 50 [12800/60000 (21%)]\tLoss: 0.000383\n",
            "Train Epoch: 50 [19200/60000 (32%)]\tLoss: 0.000030\n",
            "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 0.000583\n",
            "Train Epoch: 50 [32000/60000 (53%)]\tLoss: 0.000096\n",
            "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 0.001112\n",
            "Train Epoch: 50 [44800/60000 (75%)]\tLoss: 0.000035\n",
            "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 0.000326\n",
            "Train Epoch: 50 [57600/60000 (96%)]\tLoss: 0.000485\n",
            "Train Epoch: 51 [0/60000 (0%)]\tLoss: 0.000739\n",
            "Train Epoch: 51 [6400/60000 (11%)]\tLoss: 0.000027\n",
            "Train Epoch: 51 [12800/60000 (21%)]\tLoss: 0.000089\n",
            "Train Epoch: 51 [19200/60000 (32%)]\tLoss: 0.000595\n",
            "Train Epoch: 51 [25600/60000 (43%)]\tLoss: 0.005008\n",
            "Train Epoch: 51 [32000/60000 (53%)]\tLoss: 0.000216\n",
            "Train Epoch: 51 [38400/60000 (64%)]\tLoss: 0.000013\n",
            "Train Epoch: 51 [44800/60000 (75%)]\tLoss: 0.000368\n",
            "Train Epoch: 51 [51200/60000 (85%)]\tLoss: 0.001273\n",
            "Train Epoch: 51 [57600/60000 (96%)]\tLoss: 0.000440\n",
            "Train Epoch: 52 [0/60000 (0%)]\tLoss: 0.000325\n",
            "Train Epoch: 52 [6400/60000 (11%)]\tLoss: 0.000020\n",
            "Train Epoch: 52 [12800/60000 (21%)]\tLoss: 0.003092\n",
            "Train Epoch: 52 [19200/60000 (32%)]\tLoss: 0.000203\n",
            "Train Epoch: 52 [25600/60000 (43%)]\tLoss: 0.000144\n",
            "Train Epoch: 52 [32000/60000 (53%)]\tLoss: 0.000269\n",
            "Train Epoch: 52 [38400/60000 (64%)]\tLoss: 0.000037\n",
            "Train Epoch: 52 [44800/60000 (75%)]\tLoss: 0.000196\n",
            "Train Epoch: 52 [51200/60000 (85%)]\tLoss: 0.001033\n",
            "Train Epoch: 52 [57600/60000 (96%)]\tLoss: 0.000954\n",
            "Train Epoch: 53 [0/60000 (0%)]\tLoss: 0.000106\n",
            "Train Epoch: 53 [6400/60000 (11%)]\tLoss: 0.000287\n",
            "Train Epoch: 53 [12800/60000 (21%)]\tLoss: 0.000467\n",
            "Train Epoch: 53 [19200/60000 (32%)]\tLoss: 0.000474\n",
            "Train Epoch: 53 [25600/60000 (43%)]\tLoss: 0.000108\n",
            "Train Epoch: 53 [32000/60000 (53%)]\tLoss: 0.000746\n",
            "Train Epoch: 53 [38400/60000 (64%)]\tLoss: 0.000820\n",
            "Train Epoch: 53 [44800/60000 (75%)]\tLoss: 0.000199\n",
            "Train Epoch: 53 [51200/60000 (85%)]\tLoss: 0.000597\n",
            "Train Epoch: 53 [57600/60000 (96%)]\tLoss: 0.000095\n",
            "Train Epoch: 54 [0/60000 (0%)]\tLoss: 0.000454\n",
            "Train Epoch: 54 [6400/60000 (11%)]\tLoss: 0.000552\n",
            "Train Epoch: 54 [12800/60000 (21%)]\tLoss: 0.000752\n",
            "Train Epoch: 54 [19200/60000 (32%)]\tLoss: 0.000914\n",
            "Train Epoch: 54 [25600/60000 (43%)]\tLoss: 0.000521\n",
            "Train Epoch: 54 [32000/60000 (53%)]\tLoss: 0.000269\n",
            "Train Epoch: 54 [38400/60000 (64%)]\tLoss: 0.000148\n",
            "Train Epoch: 54 [44800/60000 (75%)]\tLoss: 0.000145\n",
            "Train Epoch: 54 [51200/60000 (85%)]\tLoss: 0.000416\n",
            "Train Epoch: 54 [57600/60000 (96%)]\tLoss: 0.003737\n",
            "Train Epoch: 55 [0/60000 (0%)]\tLoss: 0.000170\n",
            "Train Epoch: 55 [6400/60000 (11%)]\tLoss: 0.001137\n",
            "Train Epoch: 55 [12800/60000 (21%)]\tLoss: 0.005518\n",
            "Train Epoch: 55 [19200/60000 (32%)]\tLoss: 0.000268\n",
            "Train Epoch: 55 [25600/60000 (43%)]\tLoss: 0.000154\n",
            "Train Epoch: 55 [32000/60000 (53%)]\tLoss: 0.001365\n",
            "Train Epoch: 55 [38400/60000 (64%)]\tLoss: 0.000271\n",
            "Train Epoch: 55 [44800/60000 (75%)]\tLoss: 0.000242\n",
            "Train Epoch: 55 [51200/60000 (85%)]\tLoss: 0.000256\n",
            "Train Epoch: 55 [57600/60000 (96%)]\tLoss: 0.000626\n",
            "Train Epoch: 56 [0/60000 (0%)]\tLoss: 0.000217\n",
            "Train Epoch: 56 [6400/60000 (11%)]\tLoss: 0.000819\n",
            "Train Epoch: 56 [12800/60000 (21%)]\tLoss: 0.000339\n",
            "Train Epoch: 56 [19200/60000 (32%)]\tLoss: 0.000950\n",
            "Train Epoch: 56 [25600/60000 (43%)]\tLoss: 0.000267\n",
            "Train Epoch: 56 [32000/60000 (53%)]\tLoss: 0.000367\n",
            "Train Epoch: 56 [38400/60000 (64%)]\tLoss: 0.000706\n",
            "Train Epoch: 56 [44800/60000 (75%)]\tLoss: 0.000157\n",
            "Train Epoch: 56 [51200/60000 (85%)]\tLoss: 0.000015\n",
            "Train Epoch: 56 [57600/60000 (96%)]\tLoss: 0.000648\n",
            "Train Epoch: 57 [0/60000 (0%)]\tLoss: 0.000493\n",
            "Train Epoch: 57 [6400/60000 (11%)]\tLoss: 0.000108\n",
            "Train Epoch: 57 [12800/60000 (21%)]\tLoss: 0.000159\n",
            "Train Epoch: 57 [19200/60000 (32%)]\tLoss: 0.000128\n",
            "Train Epoch: 57 [25600/60000 (43%)]\tLoss: 0.000191\n",
            "Train Epoch: 57 [32000/60000 (53%)]\tLoss: 0.000716\n",
            "Train Epoch: 57 [38400/60000 (64%)]\tLoss: 0.000044\n",
            "Train Epoch: 57 [44800/60000 (75%)]\tLoss: 0.000061\n",
            "Train Epoch: 57 [51200/60000 (85%)]\tLoss: 0.000030\n",
            "Train Epoch: 57 [57600/60000 (96%)]\tLoss: 0.000153\n",
            "Train Epoch: 58 [0/60000 (0%)]\tLoss: 0.000108\n",
            "Train Epoch: 58 [6400/60000 (11%)]\tLoss: 0.000886\n",
            "Train Epoch: 58 [12800/60000 (21%)]\tLoss: 0.000391\n",
            "Train Epoch: 58 [19200/60000 (32%)]\tLoss: 0.000023\n",
            "Train Epoch: 58 [25600/60000 (43%)]\tLoss: 0.000158\n",
            "Train Epoch: 58 [32000/60000 (53%)]\tLoss: 0.000340\n",
            "Train Epoch: 58 [38400/60000 (64%)]\tLoss: 0.000083\n",
            "Train Epoch: 58 [44800/60000 (75%)]\tLoss: 0.000684\n",
            "Train Epoch: 58 [51200/60000 (85%)]\tLoss: 0.000233\n",
            "Train Epoch: 58 [57600/60000 (96%)]\tLoss: 0.000065\n",
            "Train Epoch: 59 [0/60000 (0%)]\tLoss: 0.000750\n",
            "Train Epoch: 59 [6400/60000 (11%)]\tLoss: 0.000352\n",
            "Train Epoch: 59 [12800/60000 (21%)]\tLoss: 0.000504\n",
            "Train Epoch: 59 [19200/60000 (32%)]\tLoss: 0.000126\n",
            "Train Epoch: 59 [25600/60000 (43%)]\tLoss: 0.003551\n",
            "Train Epoch: 59 [32000/60000 (53%)]\tLoss: 0.000499\n",
            "Train Epoch: 59 [38400/60000 (64%)]\tLoss: 0.000059\n",
            "Train Epoch: 59 [44800/60000 (75%)]\tLoss: 0.000147\n",
            "Train Epoch: 59 [51200/60000 (85%)]\tLoss: 0.000065\n",
            "Train Epoch: 59 [57600/60000 (96%)]\tLoss: 0.000427\n",
            "Train Epoch: 60 [0/60000 (0%)]\tLoss: 0.001659\n",
            "Train Epoch: 60 [6400/60000 (11%)]\tLoss: 0.000005\n",
            "Train Epoch: 60 [12800/60000 (21%)]\tLoss: 0.000286\n",
            "Train Epoch: 60 [19200/60000 (32%)]\tLoss: 0.000547\n",
            "Train Epoch: 60 [25600/60000 (43%)]\tLoss: 0.000099\n",
            "Train Epoch: 60 [32000/60000 (53%)]\tLoss: 0.000183\n",
            "Train Epoch: 60 [38400/60000 (64%)]\tLoss: 0.000464\n",
            "Train Epoch: 60 [44800/60000 (75%)]\tLoss: 0.000830\n",
            "Train Epoch: 60 [51200/60000 (85%)]\tLoss: 0.001896\n",
            "Train Epoch: 60 [57600/60000 (96%)]\tLoss: 0.000510\n",
            "Train Epoch: 61 [0/60000 (0%)]\tLoss: 0.000343\n",
            "Train Epoch: 61 [6400/60000 (11%)]\tLoss: 0.000076\n",
            "Train Epoch: 61 [12800/60000 (21%)]\tLoss: 0.000176\n",
            "Train Epoch: 61 [19200/60000 (32%)]\tLoss: 0.000115\n",
            "Train Epoch: 61 [25600/60000 (43%)]\tLoss: 0.000162\n",
            "Train Epoch: 61 [32000/60000 (53%)]\tLoss: 0.000016\n",
            "Train Epoch: 61 [38400/60000 (64%)]\tLoss: 0.000129\n",
            "Train Epoch: 61 [44800/60000 (75%)]\tLoss: 0.002258\n",
            "Train Epoch: 61 [51200/60000 (85%)]\tLoss: 0.000212\n",
            "Train Epoch: 61 [57600/60000 (96%)]\tLoss: 0.000335\n",
            "Train Epoch: 62 [0/60000 (0%)]\tLoss: 0.000024\n",
            "Train Epoch: 62 [6400/60000 (11%)]\tLoss: 0.000092\n",
            "Train Epoch: 62 [12800/60000 (21%)]\tLoss: 0.000347\n",
            "Train Epoch: 62 [19200/60000 (32%)]\tLoss: 0.000336\n",
            "Train Epoch: 62 [25600/60000 (43%)]\tLoss: 0.000399\n",
            "Train Epoch: 62 [32000/60000 (53%)]\tLoss: 0.000508\n",
            "Train Epoch: 62 [38400/60000 (64%)]\tLoss: 0.000619\n",
            "Train Epoch: 62 [44800/60000 (75%)]\tLoss: 0.000069\n",
            "Train Epoch: 62 [51200/60000 (85%)]\tLoss: 0.000053\n",
            "Train Epoch: 62 [57600/60000 (96%)]\tLoss: 0.000226\n",
            "Train Epoch: 63 [0/60000 (0%)]\tLoss: 0.000109\n",
            "Train Epoch: 63 [6400/60000 (11%)]\tLoss: 0.000392\n",
            "Train Epoch: 63 [12800/60000 (21%)]\tLoss: 0.000513\n",
            "Train Epoch: 63 [19200/60000 (32%)]\tLoss: 0.000024\n",
            "Train Epoch: 63 [25600/60000 (43%)]\tLoss: 0.002184\n",
            "Train Epoch: 63 [32000/60000 (53%)]\tLoss: 0.000065\n",
            "Train Epoch: 63 [38400/60000 (64%)]\tLoss: 0.000051\n",
            "Train Epoch: 63 [44800/60000 (75%)]\tLoss: 0.000233\n",
            "Train Epoch: 63 [51200/60000 (85%)]\tLoss: 0.000381\n",
            "Train Epoch: 63 [57600/60000 (96%)]\tLoss: 0.000021\n",
            "Train Epoch: 64 [0/60000 (0%)]\tLoss: 0.000782\n",
            "Train Epoch: 64 [6400/60000 (11%)]\tLoss: 0.000171\n",
            "Train Epoch: 64 [12800/60000 (21%)]\tLoss: 0.000204\n",
            "Train Epoch: 64 [19200/60000 (32%)]\tLoss: 0.000183\n",
            "Train Epoch: 64 [25600/60000 (43%)]\tLoss: 0.000957\n",
            "Train Epoch: 64 [32000/60000 (53%)]\tLoss: 0.000811\n",
            "Train Epoch: 64 [38400/60000 (64%)]\tLoss: 0.001536\n",
            "Train Epoch: 64 [44800/60000 (75%)]\tLoss: 0.000190\n",
            "Train Epoch: 64 [51200/60000 (85%)]\tLoss: 0.000102\n",
            "Train Epoch: 64 [57600/60000 (96%)]\tLoss: 0.000380\n",
            "Train Epoch: 65 [0/60000 (0%)]\tLoss: 0.000082\n",
            "Train Epoch: 65 [6400/60000 (11%)]\tLoss: 0.000151\n",
            "Train Epoch: 65 [12800/60000 (21%)]\tLoss: 0.000696\n",
            "Train Epoch: 65 [19200/60000 (32%)]\tLoss: 0.000086\n",
            "Train Epoch: 65 [25600/60000 (43%)]\tLoss: 0.003000\n",
            "Train Epoch: 65 [32000/60000 (53%)]\tLoss: 0.000470\n",
            "Train Epoch: 65 [38400/60000 (64%)]\tLoss: 0.000305\n",
            "Train Epoch: 65 [44800/60000 (75%)]\tLoss: 0.000061\n",
            "Train Epoch: 65 [51200/60000 (85%)]\tLoss: 0.000129\n",
            "Train Epoch: 65 [57600/60000 (96%)]\tLoss: 0.000010\n",
            "Train Epoch: 66 [0/60000 (0%)]\tLoss: 0.000688\n",
            "Train Epoch: 66 [6400/60000 (11%)]\tLoss: 0.000109\n",
            "Train Epoch: 66 [12800/60000 (21%)]\tLoss: 0.000188\n",
            "Train Epoch: 66 [19200/60000 (32%)]\tLoss: 0.000223\n",
            "Train Epoch: 66 [25600/60000 (43%)]\tLoss: 0.000765\n",
            "Train Epoch: 66 [32000/60000 (53%)]\tLoss: 0.000475\n",
            "Train Epoch: 66 [38400/60000 (64%)]\tLoss: 0.000336\n",
            "Train Epoch: 66 [44800/60000 (75%)]\tLoss: 0.000115\n",
            "Train Epoch: 66 [51200/60000 (85%)]\tLoss: 0.000753\n",
            "Train Epoch: 66 [57600/60000 (96%)]\tLoss: 0.003222\n",
            "Train Epoch: 67 [0/60000 (0%)]\tLoss: 0.000795\n",
            "Train Epoch: 67 [6400/60000 (11%)]\tLoss: 0.000518\n",
            "Train Epoch: 67 [12800/60000 (21%)]\tLoss: 0.000348\n",
            "Train Epoch: 67 [19200/60000 (32%)]\tLoss: 0.000592\n",
            "Train Epoch: 67 [25600/60000 (43%)]\tLoss: 0.000099\n",
            "Train Epoch: 67 [32000/60000 (53%)]\tLoss: 0.000341\n",
            "Train Epoch: 67 [38400/60000 (64%)]\tLoss: 0.000377\n",
            "Train Epoch: 67 [44800/60000 (75%)]\tLoss: 0.000069\n",
            "Train Epoch: 67 [51200/60000 (85%)]\tLoss: 0.000019\n",
            "Train Epoch: 67 [57600/60000 (96%)]\tLoss: 0.000301\n",
            "Train Epoch: 68 [0/60000 (0%)]\tLoss: 0.003457\n",
            "Train Epoch: 68 [6400/60000 (11%)]\tLoss: 0.000402\n",
            "Train Epoch: 68 [12800/60000 (21%)]\tLoss: 0.000495\n",
            "Train Epoch: 68 [19200/60000 (32%)]\tLoss: 0.000020\n",
            "Train Epoch: 68 [25600/60000 (43%)]\tLoss: 0.000140\n",
            "Train Epoch: 68 [32000/60000 (53%)]\tLoss: 0.000045\n",
            "Train Epoch: 68 [38400/60000 (64%)]\tLoss: 0.000158\n",
            "Train Epoch: 68 [44800/60000 (75%)]\tLoss: 0.000216\n",
            "Train Epoch: 68 [51200/60000 (85%)]\tLoss: 0.000012\n",
            "Train Epoch: 68 [57600/60000 (96%)]\tLoss: 0.000261\n",
            "Train Epoch: 69 [0/60000 (0%)]\tLoss: 0.000230\n",
            "Train Epoch: 69 [6400/60000 (11%)]\tLoss: 0.000019\n",
            "Train Epoch: 69 [12800/60000 (21%)]\tLoss: 0.000521\n",
            "Train Epoch: 69 [19200/60000 (32%)]\tLoss: 0.000597\n",
            "Train Epoch: 69 [25600/60000 (43%)]\tLoss: 0.000058\n",
            "Train Epoch: 69 [32000/60000 (53%)]\tLoss: 0.000126\n",
            "Train Epoch: 69 [38400/60000 (64%)]\tLoss: 0.000101\n",
            "Train Epoch: 69 [44800/60000 (75%)]\tLoss: 0.000102\n",
            "Train Epoch: 69 [51200/60000 (85%)]\tLoss: 0.000035\n",
            "Train Epoch: 69 [57600/60000 (96%)]\tLoss: 0.001221\n",
            "Train Epoch: 70 [0/60000 (0%)]\tLoss: 0.000187\n",
            "Train Epoch: 70 [6400/60000 (11%)]\tLoss: 0.001270\n",
            "Train Epoch: 70 [12800/60000 (21%)]\tLoss: 0.000118\n",
            "Train Epoch: 70 [19200/60000 (32%)]\tLoss: 0.000616\n",
            "Train Epoch: 70 [25600/60000 (43%)]\tLoss: 0.000337\n",
            "Train Epoch: 70 [32000/60000 (53%)]\tLoss: 0.000564\n",
            "Train Epoch: 70 [38400/60000 (64%)]\tLoss: 0.000278\n",
            "Train Epoch: 70 [44800/60000 (75%)]\tLoss: 0.000416\n",
            "Train Epoch: 70 [51200/60000 (85%)]\tLoss: 0.000476\n",
            "Train Epoch: 70 [57600/60000 (96%)]\tLoss: 0.001046\n",
            "Train Epoch: 71 [0/60000 (0%)]\tLoss: 0.000413\n",
            "Train Epoch: 71 [6400/60000 (11%)]\tLoss: 0.000498\n",
            "Train Epoch: 71 [12800/60000 (21%)]\tLoss: 0.000075\n",
            "Train Epoch: 71 [19200/60000 (32%)]\tLoss: 0.000310\n",
            "Train Epoch: 71 [25600/60000 (43%)]\tLoss: 0.000033\n",
            "Train Epoch: 71 [32000/60000 (53%)]\tLoss: 0.000854\n",
            "Train Epoch: 71 [38400/60000 (64%)]\tLoss: 0.000396\n",
            "Train Epoch: 71 [44800/60000 (75%)]\tLoss: 0.000053\n",
            "Train Epoch: 71 [51200/60000 (85%)]\tLoss: 0.000046\n",
            "Train Epoch: 71 [57600/60000 (96%)]\tLoss: 0.000066\n",
            "Train Epoch: 72 [0/60000 (0%)]\tLoss: 0.001756\n",
            "Train Epoch: 72 [6400/60000 (11%)]\tLoss: 0.000094\n",
            "Train Epoch: 72 [12800/60000 (21%)]\tLoss: 0.000108\n",
            "Train Epoch: 72 [19200/60000 (32%)]\tLoss: 0.000272\n",
            "Train Epoch: 72 [25600/60000 (43%)]\tLoss: 0.000027\n",
            "Train Epoch: 72 [32000/60000 (53%)]\tLoss: 0.000011\n",
            "Train Epoch: 72 [38400/60000 (64%)]\tLoss: 0.000180\n",
            "Train Epoch: 72 [44800/60000 (75%)]\tLoss: 0.000017\n",
            "Train Epoch: 72 [51200/60000 (85%)]\tLoss: 0.000146\n",
            "Train Epoch: 72 [57600/60000 (96%)]\tLoss: 0.000073\n",
            "Train Epoch: 73 [0/60000 (0%)]\tLoss: 0.000300\n",
            "Train Epoch: 73 [6400/60000 (11%)]\tLoss: 0.000923\n",
            "Train Epoch: 73 [12800/60000 (21%)]\tLoss: 0.000091\n",
            "Train Epoch: 73 [19200/60000 (32%)]\tLoss: 0.000369\n",
            "Train Epoch: 73 [25600/60000 (43%)]\tLoss: 0.000166\n",
            "Train Epoch: 73 [32000/60000 (53%)]\tLoss: 0.000014\n",
            "Train Epoch: 73 [38400/60000 (64%)]\tLoss: 0.000243\n",
            "Train Epoch: 73 [44800/60000 (75%)]\tLoss: 0.000037\n",
            "Train Epoch: 73 [51200/60000 (85%)]\tLoss: 0.000303\n",
            "Train Epoch: 73 [57600/60000 (96%)]\tLoss: 0.000321\n",
            "Train Epoch: 74 [0/60000 (0%)]\tLoss: 0.000025\n",
            "Train Epoch: 74 [6400/60000 (11%)]\tLoss: 0.001567\n",
            "Train Epoch: 74 [12800/60000 (21%)]\tLoss: 0.000268\n",
            "Train Epoch: 74 [19200/60000 (32%)]\tLoss: 0.000062\n",
            "Train Epoch: 74 [25600/60000 (43%)]\tLoss: 0.000431\n",
            "Train Epoch: 74 [32000/60000 (53%)]\tLoss: 0.001241\n",
            "Train Epoch: 74 [38400/60000 (64%)]\tLoss: 0.000068\n",
            "Train Epoch: 74 [44800/60000 (75%)]\tLoss: 0.000035\n",
            "Train Epoch: 74 [51200/60000 (85%)]\tLoss: 0.000161\n",
            "Train Epoch: 74 [57600/60000 (96%)]\tLoss: 0.000332\n",
            "Train Epoch: 75 [0/60000 (0%)]\tLoss: 0.000736\n",
            "Train Epoch: 75 [6400/60000 (11%)]\tLoss: 0.000220\n",
            "Train Epoch: 75 [12800/60000 (21%)]\tLoss: 0.000142\n",
            "Train Epoch: 75 [19200/60000 (32%)]\tLoss: 0.000316\n",
            "Train Epoch: 75 [25600/60000 (43%)]\tLoss: 0.000348\n",
            "Train Epoch: 75 [32000/60000 (53%)]\tLoss: 0.000353\n",
            "Train Epoch: 75 [38400/60000 (64%)]\tLoss: 0.000931\n",
            "Train Epoch: 75 [44800/60000 (75%)]\tLoss: 0.000003\n",
            "Train Epoch: 75 [51200/60000 (85%)]\tLoss: 0.000077\n",
            "Train Epoch: 75 [57600/60000 (96%)]\tLoss: 0.000502\n",
            "Train Epoch: 76 [0/60000 (0%)]\tLoss: 0.000023\n",
            "Train Epoch: 76 [6400/60000 (11%)]\tLoss: 0.000019\n",
            "Train Epoch: 76 [12800/60000 (21%)]\tLoss: 0.000092\n",
            "Train Epoch: 76 [19200/60000 (32%)]\tLoss: 0.000171\n",
            "Train Epoch: 76 [25600/60000 (43%)]\tLoss: 0.000565\n",
            "Train Epoch: 76 [32000/60000 (53%)]\tLoss: 0.000641\n",
            "Train Epoch: 76 [38400/60000 (64%)]\tLoss: 0.000013\n",
            "Train Epoch: 76 [44800/60000 (75%)]\tLoss: 0.000209\n",
            "Train Epoch: 76 [51200/60000 (85%)]\tLoss: 0.000640\n",
            "Train Epoch: 76 [57600/60000 (96%)]\tLoss: 0.000224\n",
            "Train Epoch: 77 [0/60000 (0%)]\tLoss: 0.000083\n",
            "Train Epoch: 77 [6400/60000 (11%)]\tLoss: 0.000105\n",
            "Train Epoch: 77 [12800/60000 (21%)]\tLoss: 0.000366\n",
            "Train Epoch: 77 [19200/60000 (32%)]\tLoss: 0.000595\n",
            "Train Epoch: 77 [25600/60000 (43%)]\tLoss: 0.000216\n",
            "Train Epoch: 77 [32000/60000 (53%)]\tLoss: 0.000012\n",
            "Train Epoch: 77 [38400/60000 (64%)]\tLoss: 0.000003\n",
            "Train Epoch: 77 [44800/60000 (75%)]\tLoss: 0.000187\n",
            "Train Epoch: 77 [51200/60000 (85%)]\tLoss: 0.000084\n",
            "Train Epoch: 77 [57600/60000 (96%)]\tLoss: 0.000261\n",
            "Train Epoch: 78 [0/60000 (0%)]\tLoss: 0.000073\n",
            "Train Epoch: 78 [6400/60000 (11%)]\tLoss: 0.000167\n",
            "Train Epoch: 78 [12800/60000 (21%)]\tLoss: 0.000675\n",
            "Train Epoch: 78 [19200/60000 (32%)]\tLoss: 0.000790\n",
            "Train Epoch: 78 [25600/60000 (43%)]\tLoss: 0.000244\n",
            "Train Epoch: 78 [32000/60000 (53%)]\tLoss: 0.000432\n",
            "Train Epoch: 78 [38400/60000 (64%)]\tLoss: 0.000519\n",
            "Train Epoch: 78 [44800/60000 (75%)]\tLoss: 0.000233\n",
            "Train Epoch: 78 [51200/60000 (85%)]\tLoss: 0.002123\n",
            "Train Epoch: 78 [57600/60000 (96%)]\tLoss: 0.000017\n",
            "Train Epoch: 79 [0/60000 (0%)]\tLoss: 0.000401\n",
            "Train Epoch: 79 [6400/60000 (11%)]\tLoss: 0.000062\n",
            "Train Epoch: 79 [12800/60000 (21%)]\tLoss: 0.000002\n",
            "Train Epoch: 79 [19200/60000 (32%)]\tLoss: 0.000039\n",
            "Train Epoch: 79 [25600/60000 (43%)]\tLoss: 0.000230\n",
            "Train Epoch: 79 [32000/60000 (53%)]\tLoss: 0.000468\n",
            "Train Epoch: 79 [38400/60000 (64%)]\tLoss: 0.001180\n",
            "Train Epoch: 79 [44800/60000 (75%)]\tLoss: 0.000246\n",
            "Train Epoch: 79 [51200/60000 (85%)]\tLoss: 0.000501\n",
            "Train Epoch: 79 [57600/60000 (96%)]\tLoss: 0.000080\n",
            "Train Epoch: 80 [0/60000 (0%)]\tLoss: 0.000620\n",
            "Train Epoch: 80 [6400/60000 (11%)]\tLoss: 0.000318\n",
            "Train Epoch: 80 [12800/60000 (21%)]\tLoss: 0.000159\n",
            "Train Epoch: 80 [19200/60000 (32%)]\tLoss: 0.000069\n",
            "Train Epoch: 80 [25600/60000 (43%)]\tLoss: 0.000259\n",
            "Train Epoch: 80 [32000/60000 (53%)]\tLoss: 0.000002\n",
            "Train Epoch: 80 [38400/60000 (64%)]\tLoss: 0.000222\n",
            "Train Epoch: 80 [44800/60000 (75%)]\tLoss: 0.000168\n",
            "Train Epoch: 80 [51200/60000 (85%)]\tLoss: 0.000110\n",
            "Train Epoch: 80 [57600/60000 (96%)]\tLoss: 0.000190\n",
            "Train Epoch: 81 [0/60000 (0%)]\tLoss: 0.000145\n",
            "Train Epoch: 81 [6400/60000 (11%)]\tLoss: 0.000000\n",
            "Train Epoch: 81 [12800/60000 (21%)]\tLoss: 0.000011\n",
            "Train Epoch: 81 [19200/60000 (32%)]\tLoss: 0.000103\n",
            "Train Epoch: 81 [25600/60000 (43%)]\tLoss: 0.000603\n",
            "Train Epoch: 81 [32000/60000 (53%)]\tLoss: 0.000458\n",
            "Train Epoch: 81 [38400/60000 (64%)]\tLoss: 0.000086\n",
            "Train Epoch: 81 [44800/60000 (75%)]\tLoss: 0.000033\n",
            "Train Epoch: 81 [51200/60000 (85%)]\tLoss: 0.000085\n",
            "Train Epoch: 81 [57600/60000 (96%)]\tLoss: 0.000231\n",
            "Train Epoch: 82 [0/60000 (0%)]\tLoss: 0.000003\n",
            "Train Epoch: 82 [6400/60000 (11%)]\tLoss: 0.002651\n",
            "Train Epoch: 82 [12800/60000 (21%)]\tLoss: 0.000011\n",
            "Train Epoch: 82 [19200/60000 (32%)]\tLoss: 0.000156\n",
            "Train Epoch: 82 [25600/60000 (43%)]\tLoss: 0.000157\n",
            "Train Epoch: 82 [32000/60000 (53%)]\tLoss: 0.000266\n",
            "Train Epoch: 82 [38400/60000 (64%)]\tLoss: 0.000330\n",
            "Train Epoch: 82 [44800/60000 (75%)]\tLoss: 0.000156\n",
            "Train Epoch: 82 [51200/60000 (85%)]\tLoss: 0.000319\n",
            "Train Epoch: 82 [57600/60000 (96%)]\tLoss: 0.000330\n",
            "Train Epoch: 83 [0/60000 (0%)]\tLoss: 0.000468\n",
            "Train Epoch: 83 [6400/60000 (11%)]\tLoss: 0.000011\n",
            "Train Epoch: 83 [12800/60000 (21%)]\tLoss: 0.000012\n",
            "Train Epoch: 83 [19200/60000 (32%)]\tLoss: 0.000572\n",
            "Train Epoch: 83 [25600/60000 (43%)]\tLoss: 0.000241\n",
            "Train Epoch: 83 [32000/60000 (53%)]\tLoss: 0.000023\n",
            "Train Epoch: 83 [38400/60000 (64%)]\tLoss: 0.000219\n",
            "Train Epoch: 83 [44800/60000 (75%)]\tLoss: 0.000328\n",
            "Train Epoch: 83 [51200/60000 (85%)]\tLoss: 0.000051\n",
            "Train Epoch: 83 [57600/60000 (96%)]\tLoss: 0.000041\n",
            "Train Epoch: 84 [0/60000 (0%)]\tLoss: 0.000072\n",
            "Train Epoch: 84 [6400/60000 (11%)]\tLoss: 0.000036\n",
            "Train Epoch: 84 [12800/60000 (21%)]\tLoss: 0.000722\n",
            "Train Epoch: 84 [19200/60000 (32%)]\tLoss: 0.000438\n",
            "Train Epoch: 84 [25600/60000 (43%)]\tLoss: 0.000267\n",
            "Train Epoch: 84 [32000/60000 (53%)]\tLoss: 0.001268\n",
            "Train Epoch: 84 [38400/60000 (64%)]\tLoss: 0.000620\n",
            "Train Epoch: 84 [44800/60000 (75%)]\tLoss: 0.000099\n",
            "Train Epoch: 84 [51200/60000 (85%)]\tLoss: 0.000017\n",
            "Train Epoch: 84 [57600/60000 (96%)]\tLoss: 0.000134\n",
            "Train Epoch: 85 [0/60000 (0%)]\tLoss: 0.001330\n",
            "Train Epoch: 85 [6400/60000 (11%)]\tLoss: 0.000021\n",
            "Train Epoch: 85 [12800/60000 (21%)]\tLoss: 0.000137\n",
            "Train Epoch: 85 [19200/60000 (32%)]\tLoss: 0.000196\n",
            "Train Epoch: 85 [25600/60000 (43%)]\tLoss: 0.000308\n",
            "Train Epoch: 85 [32000/60000 (53%)]\tLoss: 0.000067\n",
            "Train Epoch: 85 [38400/60000 (64%)]\tLoss: 0.000085\n",
            "Train Epoch: 85 [44800/60000 (75%)]\tLoss: 0.000010\n",
            "Train Epoch: 85 [51200/60000 (85%)]\tLoss: 0.000335\n",
            "Train Epoch: 85 [57600/60000 (96%)]\tLoss: 0.000080\n",
            "Train Epoch: 86 [0/60000 (0%)]\tLoss: 0.000091\n",
            "Train Epoch: 86 [6400/60000 (11%)]\tLoss: 0.000072\n",
            "Train Epoch: 86 [12800/60000 (21%)]\tLoss: 0.000144\n",
            "Train Epoch: 86 [19200/60000 (32%)]\tLoss: 0.000004\n",
            "Train Epoch: 86 [25600/60000 (43%)]\tLoss: 0.000089\n",
            "Train Epoch: 86 [32000/60000 (53%)]\tLoss: 0.000026\n",
            "Train Epoch: 86 [38400/60000 (64%)]\tLoss: 0.000036\n",
            "Train Epoch: 86 [44800/60000 (75%)]\tLoss: 0.000090\n",
            "Train Epoch: 86 [51200/60000 (85%)]\tLoss: 0.000035\n",
            "Train Epoch: 86 [57600/60000 (96%)]\tLoss: 0.000044\n",
            "Train Epoch: 87 [0/60000 (0%)]\tLoss: 0.000505\n",
            "Train Epoch: 87 [6400/60000 (11%)]\tLoss: 0.000033\n",
            "Train Epoch: 87 [12800/60000 (21%)]\tLoss: 0.000176\n",
            "Train Epoch: 87 [19200/60000 (32%)]\tLoss: 0.000470\n",
            "Train Epoch: 87 [25600/60000 (43%)]\tLoss: 0.000004\n",
            "Train Epoch: 87 [32000/60000 (53%)]\tLoss: 0.000403\n",
            "Train Epoch: 87 [38400/60000 (64%)]\tLoss: 0.000560\n",
            "Train Epoch: 87 [44800/60000 (75%)]\tLoss: 0.000079\n",
            "Train Epoch: 87 [51200/60000 (85%)]\tLoss: 0.000149\n",
            "Train Epoch: 87 [57600/60000 (96%)]\tLoss: 0.000820\n",
            "Train Epoch: 88 [0/60000 (0%)]\tLoss: 0.000308\n",
            "Train Epoch: 88 [6400/60000 (11%)]\tLoss: 0.000914\n",
            "Train Epoch: 88 [12800/60000 (21%)]\tLoss: 0.000076\n",
            "Train Epoch: 88 [19200/60000 (32%)]\tLoss: 0.000104\n",
            "Train Epoch: 88 [25600/60000 (43%)]\tLoss: 0.000180\n",
            "Train Epoch: 88 [32000/60000 (53%)]\tLoss: 0.000113\n",
            "Train Epoch: 88 [38400/60000 (64%)]\tLoss: 0.000233\n",
            "Train Epoch: 88 [44800/60000 (75%)]\tLoss: 0.000843\n",
            "Train Epoch: 88 [51200/60000 (85%)]\tLoss: 0.000082\n",
            "Train Epoch: 88 [57600/60000 (96%)]\tLoss: 0.000591\n",
            "Train Epoch: 89 [0/60000 (0%)]\tLoss: 0.000206\n",
            "Train Epoch: 89 [6400/60000 (11%)]\tLoss: 0.000108\n",
            "Train Epoch: 89 [12800/60000 (21%)]\tLoss: 0.000056\n",
            "Train Epoch: 89 [19200/60000 (32%)]\tLoss: 0.000338\n",
            "Train Epoch: 89 [25600/60000 (43%)]\tLoss: 0.000086\n",
            "Train Epoch: 89 [32000/60000 (53%)]\tLoss: 0.000072\n",
            "Train Epoch: 89 [38400/60000 (64%)]\tLoss: 0.000137\n",
            "Train Epoch: 89 [44800/60000 (75%)]\tLoss: 0.000018\n",
            "Train Epoch: 89 [51200/60000 (85%)]\tLoss: 0.000492\n",
            "Train Epoch: 89 [57600/60000 (96%)]\tLoss: 0.000280\n",
            "Train Epoch: 90 [0/60000 (0%)]\tLoss: 0.001468\n",
            "Train Epoch: 90 [6400/60000 (11%)]\tLoss: 0.000202\n",
            "Train Epoch: 90 [12800/60000 (21%)]\tLoss: 0.000041\n",
            "Train Epoch: 90 [19200/60000 (32%)]\tLoss: 0.000040\n",
            "Train Epoch: 90 [25600/60000 (43%)]\tLoss: 0.000305\n",
            "Train Epoch: 90 [32000/60000 (53%)]\tLoss: 0.000080\n",
            "Train Epoch: 90 [38400/60000 (64%)]\tLoss: 0.000042\n",
            "Train Epoch: 90 [44800/60000 (75%)]\tLoss: 0.000020\n",
            "Train Epoch: 90 [51200/60000 (85%)]\tLoss: 0.000092\n",
            "Train Epoch: 90 [57600/60000 (96%)]\tLoss: 0.000023\n",
            "Train Epoch: 91 [0/60000 (0%)]\tLoss: 0.000109\n",
            "Train Epoch: 91 [6400/60000 (11%)]\tLoss: 0.000335\n",
            "Train Epoch: 91 [12800/60000 (21%)]\tLoss: 0.000262\n",
            "Train Epoch: 91 [19200/60000 (32%)]\tLoss: 0.000032\n",
            "Train Epoch: 91 [25600/60000 (43%)]\tLoss: 0.000082\n",
            "Train Epoch: 91 [32000/60000 (53%)]\tLoss: 0.000484\n",
            "Train Epoch: 91 [38400/60000 (64%)]\tLoss: 0.000130\n",
            "Train Epoch: 91 [44800/60000 (75%)]\tLoss: 0.000053\n",
            "Train Epoch: 91 [51200/60000 (85%)]\tLoss: 0.000087\n",
            "Train Epoch: 91 [57600/60000 (96%)]\tLoss: 0.000051\n",
            "Train Epoch: 92 [0/60000 (0%)]\tLoss: 0.000932\n",
            "Train Epoch: 92 [6400/60000 (11%)]\tLoss: 0.000057\n",
            "Train Epoch: 92 [12800/60000 (21%)]\tLoss: 0.000074\n",
            "Train Epoch: 92 [19200/60000 (32%)]\tLoss: 0.000071\n",
            "Train Epoch: 92 [25600/60000 (43%)]\tLoss: 0.000009\n",
            "Train Epoch: 92 [32000/60000 (53%)]\tLoss: 0.000019\n",
            "Train Epoch: 92 [38400/60000 (64%)]\tLoss: 0.000209\n",
            "Train Epoch: 92 [44800/60000 (75%)]\tLoss: 0.000230\n",
            "Train Epoch: 92 [51200/60000 (85%)]\tLoss: 0.000097\n",
            "Train Epoch: 92 [57600/60000 (96%)]\tLoss: 0.000055\n",
            "Train Epoch: 93 [0/60000 (0%)]\tLoss: 0.000147\n",
            "Train Epoch: 93 [6400/60000 (11%)]\tLoss: 0.000471\n",
            "Train Epoch: 93 [12800/60000 (21%)]\tLoss: 0.000006\n",
            "Train Epoch: 93 [19200/60000 (32%)]\tLoss: 0.000101\n",
            "Train Epoch: 93 [25600/60000 (43%)]\tLoss: 0.000365\n",
            "Train Epoch: 93 [32000/60000 (53%)]\tLoss: 0.000388\n",
            "Train Epoch: 93 [38400/60000 (64%)]\tLoss: 0.000073\n",
            "Train Epoch: 93 [44800/60000 (75%)]\tLoss: 0.000034\n",
            "Train Epoch: 93 [51200/60000 (85%)]\tLoss: 0.000027\n",
            "Train Epoch: 93 [57600/60000 (96%)]\tLoss: 0.000064\n",
            "Train Epoch: 94 [0/60000 (0%)]\tLoss: 0.000046\n",
            "Train Epoch: 94 [6400/60000 (11%)]\tLoss: 0.000085\n",
            "Train Epoch: 94 [12800/60000 (21%)]\tLoss: 0.001087\n",
            "Train Epoch: 94 [19200/60000 (32%)]\tLoss: 0.000008\n",
            "Train Epoch: 94 [25600/60000 (43%)]\tLoss: 0.000056\n",
            "Train Epoch: 94 [32000/60000 (53%)]\tLoss: 0.000096\n",
            "Train Epoch: 94 [38400/60000 (64%)]\tLoss: 0.000220\n",
            "Train Epoch: 94 [44800/60000 (75%)]\tLoss: 0.000303\n",
            "Train Epoch: 94 [51200/60000 (85%)]\tLoss: 0.000149\n",
            "Train Epoch: 94 [57600/60000 (96%)]\tLoss: 0.000106\n",
            "Train Epoch: 95 [0/60000 (0%)]\tLoss: 0.000013\n",
            "Train Epoch: 95 [6400/60000 (11%)]\tLoss: 0.000173\n",
            "Train Epoch: 95 [12800/60000 (21%)]\tLoss: 0.000009\n",
            "Train Epoch: 95 [19200/60000 (32%)]\tLoss: 0.000027\n",
            "Train Epoch: 95 [25600/60000 (43%)]\tLoss: 0.000425\n",
            "Train Epoch: 95 [32000/60000 (53%)]\tLoss: 0.000192\n",
            "Train Epoch: 95 [38400/60000 (64%)]\tLoss: 0.000111\n",
            "Train Epoch: 95 [44800/60000 (75%)]\tLoss: 0.000155\n",
            "Train Epoch: 95 [51200/60000 (85%)]\tLoss: 0.000078\n",
            "Train Epoch: 95 [57600/60000 (96%)]\tLoss: 0.000202\n",
            "Train Epoch: 96 [0/60000 (0%)]\tLoss: 0.000079\n",
            "Train Epoch: 96 [6400/60000 (11%)]\tLoss: 0.000152\n",
            "Train Epoch: 96 [12800/60000 (21%)]\tLoss: 0.000308\n",
            "Train Epoch: 96 [19200/60000 (32%)]\tLoss: 0.000389\n",
            "Train Epoch: 96 [25600/60000 (43%)]\tLoss: 0.000056\n",
            "Train Epoch: 96 [32000/60000 (53%)]\tLoss: 0.000026\n",
            "Train Epoch: 96 [38400/60000 (64%)]\tLoss: 0.000413\n",
            "Train Epoch: 96 [44800/60000 (75%)]\tLoss: 0.000112\n",
            "Train Epoch: 96 [51200/60000 (85%)]\tLoss: 0.000216\n",
            "Train Epoch: 96 [57600/60000 (96%)]\tLoss: 0.000110\n",
            "Train Epoch: 97 [0/60000 (0%)]\tLoss: 0.000286\n",
            "Train Epoch: 97 [6400/60000 (11%)]\tLoss: 0.000167\n",
            "Train Epoch: 97 [12800/60000 (21%)]\tLoss: 0.000022\n",
            "Train Epoch: 97 [19200/60000 (32%)]\tLoss: 0.000003\n",
            "Train Epoch: 97 [25600/60000 (43%)]\tLoss: 0.000204\n",
            "Train Epoch: 97 [32000/60000 (53%)]\tLoss: 0.000046\n",
            "Train Epoch: 97 [38400/60000 (64%)]\tLoss: 0.000028\n",
            "Train Epoch: 97 [44800/60000 (75%)]\tLoss: 0.000092\n",
            "Train Epoch: 97 [51200/60000 (85%)]\tLoss: 0.000026\n",
            "Train Epoch: 97 [57600/60000 (96%)]\tLoss: 0.000013\n",
            "Train Epoch: 98 [0/60000 (0%)]\tLoss: 0.000256\n",
            "Train Epoch: 98 [6400/60000 (11%)]\tLoss: 0.000124\n",
            "Train Epoch: 98 [12800/60000 (21%)]\tLoss: 0.000061\n",
            "Train Epoch: 98 [19200/60000 (32%)]\tLoss: 0.000270\n",
            "Train Epoch: 98 [25600/60000 (43%)]\tLoss: 0.000082\n",
            "Train Epoch: 98 [32000/60000 (53%)]\tLoss: 0.000009\n",
            "Train Epoch: 98 [38400/60000 (64%)]\tLoss: 0.000119\n",
            "Train Epoch: 98 [44800/60000 (75%)]\tLoss: 0.000391\n",
            "Train Epoch: 98 [51200/60000 (85%)]\tLoss: 0.000235\n",
            "Train Epoch: 98 [57600/60000 (96%)]\tLoss: 0.000176\n",
            "Train Epoch: 99 [0/60000 (0%)]\tLoss: 0.000045\n",
            "Train Epoch: 99 [6400/60000 (11%)]\tLoss: 0.000096\n",
            "Train Epoch: 99 [12800/60000 (21%)]\tLoss: 0.000035\n",
            "Train Epoch: 99 [19200/60000 (32%)]\tLoss: 0.000424\n",
            "Train Epoch: 99 [25600/60000 (43%)]\tLoss: 0.000392\n",
            "Train Epoch: 99 [32000/60000 (53%)]\tLoss: 0.000556\n",
            "Train Epoch: 99 [38400/60000 (64%)]\tLoss: 0.000021\n",
            "Train Epoch: 99 [44800/60000 (75%)]\tLoss: 0.000008\n",
            "Train Epoch: 99 [51200/60000 (85%)]\tLoss: 0.000213\n",
            "Train Epoch: 99 [57600/60000 (96%)]\tLoss: 0.000039\n",
            "Train set: Average loss: 0.0000, Accuracy: 60000/60000 (100%)\n",
            "Test set: Average loss: 0.0006, Accuracy: 9900/10000 (99%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}